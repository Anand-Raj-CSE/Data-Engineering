{"cells": [{"cell_type": "code", "execution_count": 1, "id": "0b2e3dcf-21b4-49dd-9779-a44428cd0b0a", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "dbf93b06-dbf3-4b1c-9438-dba3abb96aeb", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/17 06:08:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n.appName('Caching in RDD').enableHiveSupport().getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "b2cc7864-7eef-41aa-9dc4-d262a0eb74d3", "metadata": {"tags": []}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-2271-m.asia-south1-b.c.t-skyline-458014-v6.internal:35943\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7ff79c87b710>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 4, "id": "68cc4830-b58e-4173-aeed-f1cc8a5938c5", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 5 items\n-rw-r--r--   2 root hadoop       5488 2025-06-02 15:26 /data/customers_100.csv\ndrwxr-xr-x   - root hadoop          0 2025-06-08 03:55 /data/write_output.csv\ndrwxr-xr-x   - root hadoop          0 2025-06-08 04:03 /data/write_output_1\ndrwxr-xr-x   - root hadoop          0 2025-06-08 04:09 /data/write_output_2\ndrwxr-xr-x   - root hadoop          0 2025-06-08 03:58 /data/write_output_another.csv\n"}], "source": "!hadoop fs -ls /data/"}, {"cell_type": "code", "execution_count": 5, "id": "8b5d1931-e4e8-477d-a785-e11989eb9818", "metadata": {"tags": []}, "outputs": [], "source": "customers_rdd = spark.sparkContext.textFile('/data/customers_100.csv')"}, {"cell_type": "code", "execution_count": 6, "id": "2ba66be6-5599-4fbd-ba33-7933da03fb2e", "metadata": {"tags": []}, "outputs": [], "source": "customers_filtered = customers_rdd.filter(lambda row:'Mumbai' in row)\ncustomers_mapped = customers_filtered.map(lambda row:(row.split(',')[0],1))\ncustomers_reduced = customers_mapped.reduceByKey(lambda x,y:x+y) "}, {"cell_type": "code", "execution_count": 7, "id": "62756592-603d-4b89-8c93-9fce6eb88459", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 0:>                                                          (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Time taken : 3.7087934017181396 seconds\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "import time\nstart_time = time.time()\ncustomers_reduced.count()\nend_time = time.time()\nprint(f\"Time taken : {end_time-start_time} seconds\")"}, {"cell_type": "code", "execution_count": 8, "id": "3a187b97-e6a3-49f1-b201-fcef3413cea4", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "9"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "customers_reduced.count()"}, {"cell_type": "markdown", "id": "50c815ec-2cf5-45b7-97c2-d345cb186838", "metadata": {}, "source": "2nd run was very fast as pyspark does caching on its own, we would now cache the data \n#### Caching is by default lazy ."}, {"cell_type": "code", "execution_count": 9, "id": "517913ea-0c88-424b-bebe-7a7c3d5973d1", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "PythonRDD[8] at RDD at PythonRDD.scala:53"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "customers_reduced.cache() # nothing would be Shown in the spark UI as caching is lazy."}, {"cell_type": "code", "execution_count": 10, "id": "59dd09bb-d441-45e7-ad1f-721b2f00c515", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "9"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "customers_reduced.count()"}, {"cell_type": "code", "execution_count": 11, "id": "3fadd217-a06c-4e42-9c69-faa1d1dcc59b", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "PythonRDD[8] at RDD at PythonRDD.scala:53"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "# uncaching\ncustomers_reduced.unpersist()"}, {"cell_type": "code", "execution_count": 12, "id": "83006cb0-6c65-426f-a050-c36c645a657f", "metadata": {"tags": []}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "4b88d526-90ff-4609-b3a1-4933250db870", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}