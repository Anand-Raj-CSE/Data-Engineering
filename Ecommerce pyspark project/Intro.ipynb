{"cells": [{"cell_type": "code", "execution_count": 1, "id": "f2dd182a-6382-420a-bb3b-9bfb10734000", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "e11c1a5a-cda4-4b42-85f0-a8fd8a5c12b8", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/07/13 07:42:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder.appName(\"Read and process data\").getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "a600f37b-9046-4ac4-8e8f-a5259640f6c9", "metadata": {"tags": []}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://ecommerce-project-m.asia-south1-b.c.t-skyline-458014-v6.internal:40305\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fcc5cdc2790>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "markdown", "id": "48f35ef4-2b2f-413c-b7a5-1e2d00be9962", "metadata": {}, "source": "## We would be working on realtime brazilian data set of ecommerce\nThsis is the link for dataset for data exploartion : https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce "}, {"cell_type": "code", "execution_count": 4, "id": "0e25ee7a-dbfc-473e-a924-3f1b4888015b", "metadata": {"tags": []}, "outputs": [], "source": "# stopping our spark as not using it as of now\nspark.stop()"}, {"cell_type": "markdown", "id": "66d735c4-c1ca-42e3-9e93-1e33117fd92c", "metadata": {}, "source": "### Project general workflow\n1. Data ingestion and exploration :  \n1.1. Setup our hadoop and spark environment  \n1.2. Import csv into hdfs  \n1.3. Load data into spark dataproc  \n1.4. Examining the schema and datatype  \n1.5. Perform EDA - Exploratory Data Analysis on top of our data  \n\n2. Data clenaing and Transformation  \n2.1. Address quality issue and trnaform the data into structered format.  \n2.2. Handling of NULL format  \n2.3. Standardize date formats - normalise and scale numeric value  \n2.4. Create new features  \n\n3. Data Integration and aggregation  \n3.1. Combine data from multiple tables to create a unified dataset  \n3.2. Perform joins and aggregate data to compute metrics  \n3.3. Resolve any data inconsistency arising from data integration  \n\n4. Performance optimization  \n4.1. Enhance data efficiency of data processing task.  \n4.2. Data Partitioning to optimize for spark jobs.  \n4.3. Utilize caching for iteration operations.  \n4.4. Optimizing the spark configuration.  \n\n5. Data serving  \n5.1. Make the processed data available for stakeholders.  \n5.2. Export transformed data to external systems/databases , for us it is hadoop.  \n5.3. Create visualizations for trends and patterns."}, {"cell_type": "code", "execution_count": null, "id": "ebb3d2c1-fd02-47fc-8253-80d91ffcf6fa", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "ec361252-db82-4ba5-a861-529570cbf6f1", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}