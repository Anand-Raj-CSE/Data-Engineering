{"cells": [{"cell_type": "code", "execution_count": 1, "id": "58e2c2a7-787e-4c5c-8a88-d1af753e6a9a", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "22dc1b23-823d-44f9-9969-2827c0219eaf", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/07 14:46:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n.appName('read mode').getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "5dc29d39-c0d5-4f55-aadc-47838e96499b", "metadata": {"tags": []}, "outputs": [], "source": "hdfs_path = '/data/customers_100.csv'"}, {"cell_type": "code", "execution_count": 4, "id": "40c7bec4-6159-4ce6-a48d-89079ff12195", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_failfast= spark.read \\\n.format('csv').option('header','true').option('inferschema','true') \\\n.option('mode','failfast').load(hdfs_path)"}, {"cell_type": "code", "execution_count": 5, "id": "b678e9e0-3905-4f08-8554-3ab803238424", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    false|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    false|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    false|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     true|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     true|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     true|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     true|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    false|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    false|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     true|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    false|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     true|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    false|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    false|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     true|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     true|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "df_failfast.show()"}, {"cell_type": "code", "execution_count": 6, "id": "33e51e3d-af6c-4278-af36-2c058b956d7c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    false|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    false|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    false|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     true|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     true|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     true|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     true|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    false|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    false|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     true|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    false|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     true|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    false|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    false|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     true|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     true|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "df_permissive= spark.read \\\n.format('csv').option('header','true').option('inferschema','true') \\\n.option('mode','permissive').load(hdfs_path)\n\ndf_permissive.show()"}, {"cell_type": "code", "execution_count": 7, "id": "bac3b33f-88f8-4402-9857-762a511ad787", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    false|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    false|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    false|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     true|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     true|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     true|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     true|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    false|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    false|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     true|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    false|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     true|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    false|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    false|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     true|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     true|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "df_dropmalformed= spark.read \\\n.format('csv').option('header','true').option('inferschema','true') \\\n.option('mode','dropmalformed').load(hdfs_path)\n\ndf_dropmalformed.show()"}, {"cell_type": "code", "execution_count": null, "id": "1a98b57e-083b-4529-a1bb-1817a1bc69a3", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}