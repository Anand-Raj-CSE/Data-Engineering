{"cells": [{"cell_type": "code", "execution_count": 1, "id": "6ade4177-cd5b-4e5e-a9f0-8557beb61df0", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "f0a5ac5c-3b87-4a41-81d4-e3d0e5289514", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 09:00:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n.appName('Spark SQL').enableHiveSupport().getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "46f86ab5-662b-41d2-bad3-6de2c6c26d38", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read.format('csv').option(\"header\",\"true\")\\\n.option(\"inferschema\",'true').load('/data/customers_100.csv') "}, {"cell_type": "code", "execution_count": 4, "id": "3719219f-3856-4740-9812-70bae26867ac", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    false|\n|          5|Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    false|\n|          6|Customer_6|     Pune|      Delhi|  India|       2023-08-29|    false|\n|          7|Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     true|\n|          8|Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     true|\n|          9|Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     true|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.show(10)"}, {"cell_type": "code", "execution_count": 5, "id": "977f2f80-d0f3-4e52-bb0b-53d09d73407c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n+---------+---------+-----------+\n\n"}], "source": "spark.sql('Show tables').show()"}, {"cell_type": "code", "execution_count": 6, "id": "52cc9b38-a810-4590-a460-4e13eb80c2c4", "metadata": {"tags": []}, "outputs": [{"ename": "AnalysisException", "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `customers_persistant` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 14;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Project [*]\n      +- 'UnresolvedRelation [customers_persistant], [], false\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mSelect * from customers_persistant limit 10\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customers_persistant` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 14;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Project [*]\n      +- 'UnresolvedRelation [customers_persistant], [], false\n"]}], "source": "spark.sql('''\nSelect * from customers_persistant limit 10\n''').show()"}, {"cell_type": "code", "execution_count": 9, "id": "ced43010-50b7-463f-a3e3-71bddbe10196", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('''\ndrop table default.customers_persistant\n'''\n).show()"}, {"cell_type": "code", "execution_count": 10, "id": "eb0a7b0e-7ab6-4266-a8a0-084d39cb2395", "metadata": {"tags": []}, "outputs": [{"ename": "AnalysisException", "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`customers_persistant` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 18;\n'DescribeRelation true, [col_name#142, data_type#143, comment#144]\n+- 'UnresolvedTableOrView [default, customers_persistant], DESCRIBE TABLE, true\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescribe extended default.customers_persistant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`customers_persistant` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 18;\n'DescribeRelation true, [col_name#142, data_type#143, comment#144]\n+- 'UnresolvedTableOrView [default, customers_persistant], DESCRIBE TABLE, true\n"]}], "source": "spark.sql('describe extended default.customers_persistant').show(truncate=False)"}, {"cell_type": "code", "execution_count": 11, "id": "8b961f27-8683-4be9-999d-8659a72bf41e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n+---------+---------+-----------+\n\n"}], "source": "spark.sql('Show tables').show()"}, {"cell_type": "code", "execution_count": 12, "id": "64579d1d-4e9c-4af6-9bea-ab637b3b6e64", "metadata": {"tags": []}, "outputs": [], "source": "df.createGlobalTempView('temp_customers')"}, {"cell_type": "code", "execution_count": 14, "id": "3759dc12-7c73-4af4-ac78-60a01d3631d7", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n+---------+---------+-----------+\n\n"}], "source": "spark.sql('Show Tables').show()"}, {"cell_type": "code", "execution_count": 15, "id": "1b7cdc57-8511-4290-b6b0-96036422ada7", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+--------------+-----------+\n|  namespace|     tableName|isTemporary|\n+-----------+--------------+-----------+\n|global_temp|temp_customers|       true|\n+-----------+--------------+-----------+\n\n"}], "source": "spark.sql('Show Tables in global_temp').show()"}, {"cell_type": "code", "execution_count": 18, "id": "8ac249ea-8c8e-4d25-971b-ed1b0fa66571", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+---------+-------+\n|col_name         |data_type|comment|\n+-----------------+---------+-------+\n|customer_id      |int      |NULL   |\n|name             |string   |NULL   |\n|city             |string   |NULL   |\n|state            |string   |NULL   |\n|country          |string   |NULL   |\n|registration_date|date     |NULL   |\n|is_active        |boolean  |NULL   |\n+-----------------+---------+-------+\n\n"}], "source": "spark.sql('''\ndescribe extended global_temp.temp_customers\n''').show(truncate=False)"}, {"cell_type": "code", "execution_count": 19, "id": "79621c28-118f-48af-8779-d551565e7edc", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 08:48:55 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}], "source": "df.write.mode('overwrite').saveAsTable('customers_persistant')"}, {"cell_type": "code", "execution_count": 1, "id": "c5ebd7d0-4fa2-48c1-806d-13c7f987ebd6", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|customers_persistant|      false|\n+---------+--------------------+-----------+\n\n"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 2, "id": "f1838195-e154-49af-84a6-52a13a4b5aac", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|customer_id                 |int                                                           |NULL   |\n|name                        |string                                                        |NULL   |\n|city                        |string                                                        |NULL   |\n|state                       |string                                                        |NULL   |\n|country                     |string                                                        |NULL   |\n|registration_date           |date                                                          |NULL   |\n|is_active                   |boolean                                                       |NULL   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Catalog                     |spark_catalog                                                 |       |\n|Database                    |default                                                       |       |\n|Table                       |customers_persistant                                          |       |\n|Owner                       |root                                                          |       |\n|Created Time                |Sun Jun 15 08:48:56 UTC 2025                                  |       |\n|Last Access                 |UNKNOWN                                                       |       |\n|Created By                  |Spark 3.5.3                                                   |       |\n|Type                        |MANAGED                                                       |       |\n|Provider                    |parquet                                                       |       |\n|Statistics                  |3625 bytes                                                    |       |\n|Location                    |hdfs://cluster-2271-m/user/hive/warehouse/customers_persistant|       |\n+----------------------------+--------------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('DESCRIBE EXTENDED customers_persistant').show(truncate=False)"}, {"cell_type": "markdown", "id": "84170f89-2314-4f01-8ecd-ef56a678ac79", "metadata": {}, "source": "### See ther are two types of tables - MANAGED AND EXTERNAL TABLE , in managed tables , the metadata and data is both owned by Spark. In external data only metadata is owned , data is stored externally."}, {"cell_type": "code", "execution_count": 3, "id": "03531f2c-9d88-4d0e-a473-75f1e26cfaf9", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql(\"drop table customers_persistant\").show()"}, {"cell_type": "code", "execution_count": 7, "id": "dc948820-91f6-4949-8376-80d73985d68c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    false|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\nonly showing top 5 rows\n\n"}], "source": "# Creating a temp table\ndf.show(5)"}, {"cell_type": "code", "execution_count": 8, "id": "d06e6128-215e-4bd2-ae8d-a451c75f8a22", "metadata": {"tags": []}, "outputs": [], "source": "df.createOrReplaceTempView('temp_cust')"}, {"cell_type": "code", "execution_count": 10, "id": "690c4c62-5e72-47ec-b6d5-52e5b1ef41df", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n|         |temp_cust|       true|\n+---------+---------+-----------+\n\n"}], "source": "spark.sql('Show Tables').show()"}, {"cell_type": "markdown", "id": "0efa5a37-657b-4363-aca0-c36c5ba220f9", "metadata": {}, "source": "We could create a table from extisting table like following cell "}, {"cell_type": "code", "execution_count": 11, "id": "4776c9c9-bdc3-4cbb-8d3e-a1b5bc88c9ab", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 09:04:11 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n25/06/15 09:04:12 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n                                                                                \r"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('''\ncreate table managed_customers as select * from temp_cust\n''')"}, {"cell_type": "code", "execution_count": 12, "id": "a98c6726-809d-44d7-8c55-d215d2c48756", "metadata": {"tags": []}, "outputs": [], "source": "def show_table():\n    spark.sql('Show Tables').show()"}, {"cell_type": "code", "execution_count": 13, "id": "67c7fc05-099f-4e6c-add1-fcf47d974b25", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+-----------------+-----------+\n|namespace|        tableName|isTemporary|\n+---------+-----------------+-----------+\n|  default|managed_customers|      false|\n|         |        temp_cust|       true|\n+---------+-----------------+-----------+\n\n"}], "source": "show_table()"}, {"cell_type": "code", "execution_count": 15, "id": "f9eba9ba-d57b-431b-9a5b-6f58822d61b3", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+----------------------------------+-------+\n|col_name                    |data_type                         |comment|\n+----------------------------+----------------------------------+-------+\n|customer_id                 |int                               |NULL   |\n|name                        |string                            |NULL   |\n|city                        |string                            |NULL   |\n|state                       |string                            |NULL   |\n|country                     |string                            |NULL   |\n|registration_date           |date                              |NULL   |\n|is_active                   |boolean                           |NULL   |\n|                            |                                  |       |\n|# Detailed Table Information|                                  |       |\n|Catalog                     |spark_catalog                     |       |\n|Database                    |default                           |       |\n|Table                       |managed_customers                 |       |\n|Owner                       |root                              |       |\n|Created Time                |Sun Jun 15 09:04:12 UTC 2025      |       |\n|Last Access                 |UNKNOWN                           |       |\n|Created By                  |Spark 3.5.3                       |       |\n|Type                        |MANAGED                           |       |\n|Provider                    |hive                              |       |\n|Table Properties            |[transient_lastDdlTime=1749978254]|       |\n|Statistics                  |5424 bytes                        |       |\n+----------------------------+----------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('DESCRIBE EXTENDED managed_customers').show(truncate=False)"}, {"cell_type": "code", "execution_count": 16, "id": "490bda4f-378d-4b51-ba58-e242c1157839", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|         customer_id|                 int|   NULL|\n|                name|              string|   NULL|\n|                city|              string|   NULL|\n|               state|              string|   NULL|\n|             country|              string|   NULL|\n|   registration_date|                date|   NULL|\n|           is_active|             boolean|   NULL|\n|                    |                    |       |\n|# Detailed Table ...|                    |       |\n|             Catalog|       spark_catalog|       |\n|            Database|             default|       |\n|               Table|   managed_customers|       |\n|               Owner|                root|       |\n|        Created Time|Sun Jun 15 09:04:...|       |\n|         Last Access|             UNKNOWN|       |\n|          Created By|         Spark 3.5.3|       |\n|                Type|             MANAGED|       |\n|            Provider|                hive|       |\n|    Table Properties|[transient_lastDd...|       |\n|          Statistics|          5424 bytes|       |\n+--------------------+--------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('Describe extended managed_customers').show()"}, {"cell_type": "code", "execution_count": 17, "id": "f8febd6a-2d0f-44c7-b9b5-d640d6c144ee", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\ndrwxr-xr-x   - root hadoop          0 2025-06-15 09:04 /user/hive/warehouse/managed_customers\n"}], "source": "!hadoop fs -ls /user/hive/warehouse/  "}, {"cell_type": "markdown", "id": "ee209c8a-53db-4309-bc02-7720d8c21ef3", "metadata": {}, "source": "### creating external tables"}, {"cell_type": "code", "execution_count": 18, "id": "089f2643-cec1-4a41-a52d-7cbc76b16711", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 5 items\n-rw-r--r--   2 root hadoop       5488 2025-06-02 15:26 /data/customers_100.csv\ndrwxr-xr-x   - root hadoop          0 2025-06-08 03:55 /data/write_output.csv\ndrwxr-xr-x   - root hadoop          0 2025-06-08 04:03 /data/write_output_1\ndrwxr-xr-x   - root hadoop          0 2025-06-08 04:09 /data/write_output_2\ndrwxr-xr-x   - root hadoop          0 2025-06-08 03:58 /data/write_output_another.csv\n"}], "source": "!hadoop fs -ls /data/"}, {"cell_type": "code", "execution_count": 19, "id": "dff02963-a8b1-4d65-bdc1-e7b4f8958adc", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 09:17:06 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`external_customers` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "# Creating external table\nspark.sql('''\ncreate external table external_customers\nusing csv\nlocation '/data/customers_100.csv'\n''')"}, {"cell_type": "code", "execution_count": 20, "id": "9fb2ae34-ee27-4de6-91fc-2131388ffee7", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------+-------+\n|col_name                    |data_type                                         |comment|\n+----------------------------+--------------------------------------------------+-------+\n|_c0                         |string                                            |NULL   |\n|_c1                         |string                                            |NULL   |\n|_c2                         |string                                            |NULL   |\n|_c3                         |string                                            |NULL   |\n|_c4                         |string                                            |NULL   |\n|_c5                         |string                                            |NULL   |\n|_c6                         |string                                            |NULL   |\n|                            |                                                  |       |\n|# Detailed Table Information|                                                  |       |\n|Catalog                     |spark_catalog                                     |       |\n|Database                    |default                                           |       |\n|Table                       |external_customers                                |       |\n|Owner                       |root                                              |       |\n|Created Time                |Sun Jun 15 09:17:06 UTC 2025                      |       |\n|Last Access                 |UNKNOWN                                           |       |\n|Created By                  |Spark 3.5.3                                       |       |\n|Type                        |EXTERNAL                                          |       |\n|Provider                    |csv                                               |       |\n|Location                    |hdfs://cluster-2271-m/data/customers_100.csv      |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n+----------------------------+--------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('Describe extended external_customers').show(truncate=False)"}, {"cell_type": "code", "execution_count": 21, "id": "7d7c7d2a-740d-428d-9abf-7264d1b7b35e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "customer_id,name,city,state,country,registration_date,is_active\n0,Customer_0,Pune,Maharashtra,India,2023-06-29,False\n1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True\n2,Customer_2,Hyderabad,Gujarat,India,2023-10-27,True\n3,Customer_3,Bangalore,Karnataka,India,2023-10-17,False\n4,Customer_4,Ahmedabad,Karnataka,India,2023-03-14,False\n5,Customer_5,Hyderabad,Karnataka,India,2023-07-28,False\n6,Customer_6,Pune,Delhi,India,2023-08-29,False\n7,Customer_7,Ahmedabad,West Bengal,India,2023-12-28,True\n8,Customer_8,Pune,Karnataka,India,2023-06-22,True\n9,Customer_9,Mumbai,Telangana,India,2023-01-05,True\n10,Customer_10,Pune,Gujarat,India,2023-08-05,True\n11,Customer_11,Delhi,West Bengal,India,2023-08-02,False\n12,Customer_12,Chennai,Gujarat,India,2023-11-21,False\n13,Customer_13,Chennai,Karnataka,India,2023-11-06,True\n14,Customer_14,Hyderabad,Tamil Nadu,India,2023-02-07,False\n15,Customer_15,Mumbai,Gujarat,India,2023-03-02,True\n16,Customer_16,Chennai,Karnataka,India,2023-04-05,False\n17,Customer_17,Hyderabad,West Bengal,India"}], "source": "# Creating table using schema\n!hadoop fs -head /data/customers_100.csv"}, {"cell_type": "code", "execution_count": 22, "id": "9dba04f7-5cb2-4950-9f7a-735314ff1da5", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 09:23:44 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`external_cust` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('''\nCREATE EXTERNAL TABLE external_cust (\ncustomer_id INT,\nname STRING,\ncity STRING,\nstate STRING,\ncountry STRING,\nregistration_date STRING,\nis_active BOOLEAN\n)\nUSING csv\nLOCATION '/data/customers_100.csv'\n''')"}, {"cell_type": "code", "execution_count": 23, "id": "80c6a964-8fab-41a8-97ce-085d5ec3dda9", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|       NULL|      name|     city|      state|country|registration_date|     NULL|\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('Select * from external_cust limit 5').show()"}, {"cell_type": "code", "execution_count": 1, "id": "b6123fd0-bebc-4d1b-9f14-23f449d3ae19", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------+-------+\n|col_name                    |data_type                                         |comment|\n+----------------------------+--------------------------------------------------+-------+\n|customer_id                 |int                                               |NULL   |\n|name                        |string                                            |NULL   |\n|city                        |string                                            |NULL   |\n|state                       |string                                            |NULL   |\n|country                     |string                                            |NULL   |\n|registration_date           |string                                            |NULL   |\n|is_active                   |boolean                                           |NULL   |\n|                            |                                                  |       |\n|# Detailed Table Information|                                                  |       |\n|Catalog                     |spark_catalog                                     |       |\n|Database                    |default                                           |       |\n|Table                       |external_cust                                     |       |\n|Owner                       |root                                              |       |\n|Created Time                |Sun Jun 15 09:23:45 UTC 2025                      |       |\n|Last Access                 |UNKNOWN                                           |       |\n|Created By                  |Spark 3.5.3                                       |       |\n|Type                        |EXTERNAL                                          |       |\n|Provider                    |csv                                               |       |\n|Location                    |hdfs://cluster-2271-m/data/customers_100.csv      |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n+----------------------------+--------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('Describe extended external_cust').show(truncate=False)"}, {"cell_type": "code", "execution_count": 2, "id": "1232b73a-8ec6-4eae-9d0d-629acc735eaf", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\ndrwxr-xr-x   - root hadoop          0 2025-06-15 09:04 /user/hive/warehouse/managed_customers\n"}], "source": "# see type is external\n!hadoop fs -ls /user/hive/warehouse/"}, {"cell_type": "code", "execution_count": 3, "id": "75e76145-1116-4932-aa9a-db1dc652dec8", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+------------------+-----------+\n|namespace|         tableName|isTemporary|\n+---------+------------------+-----------+\n|  default|     external_cust|      false|\n|  default|external_customers|      false|\n|  default| managed_customers|      false|\n+---------+------------------+-----------+\n\n"}], "source": "spark.sql('SHOW TABLES').show()"}, {"cell_type": "code", "execution_count": 4, "id": "04f5d4d4-7028-4212-9a32-dbb267c6e330", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('DROP TABLE managed_customers').show()"}, {"cell_type": "code", "execution_count": 5, "id": "e17e5a6d-022a-4d68-9d90-f2adf8079ff3", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('DROP TABLE external_cust').show()"}, {"cell_type": "code", "execution_count": 6, "id": "634e9103-282a-4f9b-9641-152b5c1bc1b3", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('DROP TABLE external_customers').show()"}, {"cell_type": "code", "execution_count": 7, "id": "f84577d2-c948-4e7b-9faa-e3e1b3bcae59", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n+---------+---------+-----------+\n\n"}], "source": "spark.sql('SHOW TABLES').show()"}, {"cell_type": "code", "execution_count": 8, "id": "66ceb423-95fe-4bc4-969d-dd768dcd0a2c", "metadata": {"tags": []}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "b4cb1468-2320-4c23-a041-9fe0d3ee751d", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}